{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "from pygame import mixer\n",
    "import threading\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(input_image, model):\n",
    "    # Convert image from BGR to RGB\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "    # Make prediction\n",
    "    detection_results = model.process(input_image)\n",
    "    # Convert image back to BGR\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return input_image, detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(input_image, detection_results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(input_image, detection_results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(input_image, detection_results):\n",
    "    # Set drawing specifications for face connections\n",
    "    face_points = mp_drawing.DrawingSpec(color=(0,0,0), thickness=1, circle_radius=1) \n",
    "    face_lines = mp_drawing.DrawingSpec(color=(255,255,255), thickness=1)\n",
    "\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(input_image, detection_results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, face_points, face_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keypoints into flattened arrays\n",
    "def extract_keypoints (detection_results):\n",
    "\n",
    "    face = np.array([[results.x, results.y, results.z]\n",
    "    for results in detection_results.face_landmarks.landmark]).flatten() if detection_results.face_landmarks else np.zeros(468*3)\n",
    "\n",
    "    return np.concatenate([face])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store exported np array\n",
    "DATA_PATH = os.path.join('EmotionData')\n",
    "\n",
    "# Gesture that will be detected \n",
    "emotions = np.array(['Happy','Sad','Angry','Neutral','Surprised'])\n",
    "\n",
    "length_frames = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('My_model_ED.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixer.init()\n",
    "pygame.init()\n",
    "\n",
    "def play_text(text):\n",
    "    def speak_text(text):\n",
    "        try:\n",
    "            engine = pyttsx3.init()\n",
    "            engine.say(text)\n",
    "            engine.runAndWait()\n",
    "        except Exception as e:\n",
    "            print(f\"Error speaking text: {e}\")\n",
    "\n",
    "    speech_thread = threading.Thread(target=speak_text, args=(text,))\n",
    "    speech_thread.start()\n",
    "\n",
    "def play_and_delete_audio(unique_filename):\n",
    "    try:\n",
    "        mixer.music.load(unique_filename)\n",
    "        mixer.music.play()\n",
    "        while mixer.music.get_busy():\n",
    "            time.sleep(0.1)\n",
    "        os.remove(unique_filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in play_and_delete_audio: {e}\")\n",
    "\n",
    "def moving_average(predictions, window_size=3):\n",
    "    if len(predictions) < window_size:\n",
    "        return predictions\n",
    "    return [np.mean(predictions[: i + 1]) for i in range(window_size - 1)] + [\n",
    "        np.mean(predictions[i - window_size + 1 : i + 1])\n",
    "        for i in range(window_size - 1, len(predictions))\n",
    "    ]\n",
    "\n",
    "\n",
    "sequence = []\n",
    "current_emotion = []\n",
    "predictions = []\n",
    "\n",
    "threshold = 0.6\n",
    "window_size = 5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    prev_frame_time = 0\n",
    "    prediction_frequency = 1\n",
    "    frame_counter = 0\n",
    "    while cap.isOpened():\n",
    "        ret, input_frame = cap.read()\n",
    "\n",
    "        input_image, detection_results = mediapipe_detection(input_frame, holistic)\n",
    "        draw_styled_landmarks(input_image, detection_results)\n",
    "\n",
    "        keypoints = extract_keypoints(detection_results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-2:]\n",
    "\n",
    "        if len(sequence) == 2 and frame_counter % prediction_frequency == 0:\n",
    "            result = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(result))\n",
    "\n",
    "            if len(predictions) >= window_size:\n",
    "                smoothed_predictions = moving_average(predictions[-window_size:], window_size)\n",
    "\n",
    "                if len(current_emotion) > 0:\n",
    "                    if (\n",
    "                        np.unique(smoothed_predictions)[-1] == np.argmax(result)\n",
    "                        and result[np.argmax(result)] > threshold\n",
    "                    ) and emotions[np.argmax(result)] != current_emotion[-1]:\n",
    "                        current_emotion.append(emotions[np.argmax(result)])\n",
    "                        play_text(emotions[np.argmax(result)])\n",
    "                elif (\n",
    "                    np.unique(smoothed_predictions)[-1] == np.argmax(result)\n",
    "                    and result[np.argmax(result)] > threshold\n",
    "                ):\n",
    "                    current_emotion.append(emotions[np.argmax(result)])\n",
    "                    play_text(emotions[np.argmax(result)])\n",
    "\n",
    "                if len(current_emotion) > 1: \n",
    "                    current_emotion = current_emotion[-1:]\n",
    "        frame_counter += 1\n",
    "\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        fps_text = f\"FPS: {int(fps)}\"\n",
    "        cv2.putText(input_image, fps_text, (input_frame.shape[1] - 80, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        # Display the recognized emotion with a background\n",
    "        emotion_text = ' '.join(current_emotion)\n",
    "        (text_width, text_height), _ = cv2.getTextSize(emotion_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        cv2.rectangle(input_image, (5, 35 - text_height - 10), (5 + text_width + 10, 35 + 10), (0, 20, 0), -1)\n",
    "        cv2.putText(input_image, emotion_text, (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show\n",
    "        cv2.imshow('Face Emotion Recognition', input_image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    mixer.quit()\n",
    "    pygame.quit()\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
